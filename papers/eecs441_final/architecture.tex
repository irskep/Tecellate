\section{System Architecture}

The system can operate in one of two modes. In local mode, the simulation runs in one process with a
scenario defined in code. The remote mode is described below.

\subsection{Distributed Architecture}

\begin{figure*}
    \begin{center}
        \includegraphics[width=3in]{figures/arch0.pdf}
    \end{center}
    \caption{Basic architecture of the system.}
    \label{arch}
\end{figure*}

In order to support simulations on the scale of millions of agents, Tecellate can be run in a
distributed mode on many machines. Individual components communicate via TCP. The architecture of
this distributed version is shown in figure \ref{arch}.

\begin{figure*}
    \begin{center}
        \includegraphics[width=3in]{figures/arch1.png}
    \end{center}
    \caption{Grid-based method for distributing work across coordinators}
    \label{geoarch}
\end{figure*}

Agents are run in individual processes. In order to interact with the simulation, they connect to a
\textbf{coordinator}. Coordinators are responsible for evaluating discrete grid sections each turn
and communicating their results to each other when relevant. The organization of a square grid
section is shown in figure \ref{geoarch}.

The simulation starts with all agent and coordinator processes waiting and listening at individual
addresses. A master process reads a configuration file with their addresses and initial states,
sends information about how to connect to each other, waits for the connections to finish, and
instructs the coordinators to begin the simulation. It then exits and the other processes log
simulation data to configurable log files.

Each coordinator is responsible for a rectangular section of the grid. These sections are assigned
at the start of the simulation and do not change. Coordinators are connected with neighbor
coordinators that share adjacent grid sections. When an agent moves from one grid section to
another, the agent connects to the coordinator responsible for the new section.

\subsection{Coordinator Implementation Details}

There are two main types of threads associated with each coordinator. One kind of thread (A-thread)
requests agent data from neighbors, and then applies the simulation rules to the agents's previous
states based on their requested actions and the states of the agents at the neighbor coordinators.
There is only one of these threads. The other kind of thread (B-thread) serves requests for agent
information from neighbors, and there is one of these for each neighbor ($|N|$).

The A-thread and the B-threads communicate using two semaphores, \textbf{turnAvailable} and
\textbf{requestsServed}. The A-thread signals \textbf{turnAvailable} $|N|$ times when a turn has
been processed, and the B-threads each wait on it before serving a new request. When a B-thread has
served a request, it signals \textbf{requestsServed}. The A-thread waits on \textbf{requestsServed}
before it begins processing a new turn to avoid overwriting data that B-threads are still serving.
The system starts with \textbf{turnAvailable} unlocked for the B-threads.

This system allows processing and RPC serving to run in parallel, with processing running one step
ahead of RPC serving.

Processing a turn (in the A-thread) involves exchanging messages with each agent to get its moves
for the next turn. This may happen over TCP or in-memory communication channels depending on the
mode of operation.

\subsection{Issues and Possible Improvements}

The biggest issue with the current implementation is that distributed communication via TCP is
orders of magnitude slower than communication in memory. While this limiation is not surprising, it
is probably possible to improve communication speed by switching to a UDP-based protocol.

The application is highly multithreaded even within individual components. A modest run will have 40
or more threads. These are not OS-level threads but rather lightweight runtime-level threads.
Despite their lightweight nature, a large proportion are waiting for input most of the time, for
example to serve an RPC request. Our educated guess is that 60\% of threads are in this state at any
given moment. These threads may be wasting more CPU cycles than necessary when they are scheduled.

Performance improves when the language runtime is instructed to split the threads across multiple
CPU cores, but when this is done, a major memory leak manifests which makes the system unusable.
This memory leak may be caused by the language runtime, but we are still investigating the issue.

If agents cluster in one geographic area, one coordinator may have a significantly higher burden
than the others. This problem could be solved by allowing the master process to dynamically
reallocate the space assigned to each coordinator. This dynamic reassignment is very similar to the
problem of assigning many application instances to many servers, but in addition to simply trying to
distribute CPU and memory load evenly, we are also trying keep traffic between coordinators to a
minimum. It would take another paper to describe an effective algorithm to accomplish this goal, and
probably a simulator-simulator to test it.

\subsection{Use of the Go Programming Language}

This project would have been much more difficult without the use of the new programming language Go.
First released by Google in November 2009 \cite{go_new}, Go is a systems programming language with
C-like syntax, garbage collection, concurrency primitives, basic object orientation, and lightweight
threads. This code example shows the unique features of Go that made Tecellate easier to write:

\begin{tscode}
\begin{verbatim}
package main

import "fmt"

func main() {
    // Declare two "channels", one for strings (for communication) and 
    //      another one to serve as a mutex to allow the main thread 
    //      to wait until the other threads finish before exiting
    comm := make(chan string)
    can_exit := make(chan bool)
    
    // Start a thread that sends a message down the communication channel
    go func(send_chan chan string) {
        send_chan <- "hello"
    }(comm)
    
    // Start another thread that receives the message and tells the
    //      main thread to exit
    go func(recv_chan chan string, done chan bool) {
        val := <- recv_chan
        fmt.Printf("Received from sender: '%v'\n", val)
        done <- true
    }(comm, can_exit)
    
    // Wait for the receive thread to signal exit
    <- can_exit
}
\end{verbatim}
\end{tscode}

This code snippet demonstrates \textbf{goroutines} and \textbf{channels}. A goroutine is a
lightweight thread that is scheduled by the runtime. A channel is a thread-safe way to pass data
from one point to another. The code snippet demonstrates the use of one channel to pass a string to
display, and another channel to act as a mutex to stop the main thread from exiting before all work
is complete.

The standard library provides a way to attach a channel to a TCP connection. This library is what
allows us to easily switch between local and distributed modes of operation.
